{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120005,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %%capture\n# %pip install -U transformers \n# %pip install -U datasets \n# %pip install -U accelerate \n# %pip install -U peft \n# %pip install -U trl \n# %pip install -U bitsandbytes \n# %pip install -U wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:58:59.308014Z","iopub.execute_input":"2024-10-18T13:58:59.308402Z","iopub.status.idle":"2024-10-18T13:58:59.312895Z","shell.execute_reply.started":"2024-10-18T13:58:59.308367Z","shell.execute_reply":"2024-10-18T13:58:59.311949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, TextStreamer\nimport torch\n\n\nbase_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    return_dict=True,\n    low_cpu_mem_usage=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\nif model.config.pad_token_id is None:\n    model.config.pad_token_id = model.config.eos_token_id\n\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:00.288089Z","iopub.execute_input":"2024-10-18T13:59:00.289006Z","iopub.status.idle":"2024-10-18T13:59:07.366537Z","shell.execute_reply.started":"2024-10-18T13:59:00.288967Z","shell.execute_reply":"2024-10-18T13:59:07.365501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\": \"Who is Vincent van Gogh?\"}]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n\noutputs = pipe(prompt, max_new_tokens=120, do_sample=True)\n\nprint(outputs[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:07.369563Z","iopub.execute_input":"2024-10-18T13:59:07.369985Z","iopub.status.idle":"2024-10-18T13:59:13.541490Z","shell.execute_reply.started":"2024-10-18T13:59:07.369941Z","shell.execute_reply":"2024-10-18T13:59:13.540262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a skilled Python developer specializing in database management and optimization.\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I'm experiencing a sorting issue in my database. Could you please provide Python code to help resolve this problem?\",\n    },\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n\noutputs = pipe(prompt, max_new_tokens=512, do_sample=True)\n\ndisplay(\n    Markdown(\n            outputs[0][\"generated_text\"].split(\n                \"<|start_header_id|>assistant<|end_header_id|>\"\n            )[1]\n        )\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:13.542896Z","iopub.execute_input":"2024-10-18T13:59:13.543260Z","iopub.status.idle":"2024-10-18T13:59:28.042838Z","shell.execute_reply.started":"2024-10-18T13:59:13.543225Z","shell.execute_reply":"2024-10-18T13:59:28.041735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HF\")\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3.2 Chatbod-Mode-Enhancer V2.1', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)\n\nbase_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model = \"llama-3.2-3b-it-chatbot-mode-enhancer-v3\"\ndataset_name = \"NickyNicky/nlp-mental-health-conversations\"\n\n# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"\n\n\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n\n#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:28.044150Z","iopub.execute_input":"2024-10-18T13:59:28.044516Z","iopub.status.idle":"2024-10-18T13:59:43.191480Z","shell.execute_reply.started":"2024-10-18T13:59:28.044476Z","shell.execute_reply":"2024-10-18T13:59:43.190520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=65).select(range(1500)) # Only use 1500 samples for quick demo\ninstruction = \"\"\"Offer empathetic, supportive responses.\n        Validate emotions, provide gentle guidance, and encourage small positive steps.\n    \"\"\"\ndef format_chat_template(row):\n    \n    row_json = [\n        {\"role\": \"system\", \"content\": instruction },\n        {\"role\": \"user\", \"content\": row[\"Context\"]},\n        {\"role\": \"assistant\", \"content\": row[\"Response\"]}\n    ]\n    \n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:43.194625Z","iopub.execute_input":"2024-10-18T13:59:43.195102Z","iopub.status.idle":"2024-10-18T13:59:43.440815Z","shell.execute_reply.started":"2024-10-18T13:59:43.195055Z","shell.execute_reply":"2024-10-18T13:59:43.439878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:43.442437Z","iopub.execute_input":"2024-10-18T13:59:43.442840Z","iopub.status.idle":"2024-10-18T13:59:43.455693Z","shell.execute_reply.started":"2024-10-18T13:59:43.442776Z","shell.execute_reply":"2024-10-18T13:59:43.454264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_test_split = dataset.train_test_split(test_size=0.15)\n\n# Accessing the train and test datasets\ntrain_dataset = train_test_split['train']\ntest_dataset = train_test_split['test']\n\n# Verifying the number of rows in each split\nprint(f\"Train dataset rows: {train_dataset.num_rows}\")\nprint(f\"Test dataset rows: {test_dataset.num_rows}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:43.457272Z","iopub.execute_input":"2024-10-18T13:59:43.457640Z","iopub.status.idle":"2024-10-18T13:59:43.477045Z","shell.execute_reply.started":"2024-10-18T13:59:43.457597Z","shell.execute_reply":"2024-10-18T13:59:43.476067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:43.478568Z","iopub.execute_input":"2024-10-18T13:59:43.478939Z","iopub.status.idle":"2024-10-18T13:59:43.488394Z","shell.execute_reply.started":"2024-10-18T13:59:43.478897Z","shell.execute_reply":"2024-10-18T13:59:43.485293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)\n\n#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:43.489976Z","iopub.execute_input":"2024-10-18T13:59:43.490339Z","iopub.status.idle":"2024-10-18T13:59:44.318884Z","shell.execute_reply.started":"2024-10-18T13:59:43.490295Z","shell.execute_reply":"2024-10-18T13:59:44.317761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=3,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=100,\n    logging_strategy=\"steps\",\n    learning_rate=5e-6,\n    fp16=True,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n\n# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:44.320077Z","iopub.execute_input":"2024-10-18T13:59:44.321077Z","iopub.status.idle":"2024-10-18T13:59:46.426747Z","shell.execute_reply.started":"2024-10-18T13:59:44.321040Z","shell.execute_reply":"2024-10-18T13:59:46.425977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:46.427791Z","iopub.execute_input":"2024-10-18T13:59:46.428141Z","iopub.status.idle":"2024-10-18T13:59:49.099697Z","shell.execute_reply.started":"2024-10-18T13:59:46.428108Z","shell.execute_reply":"2024-10-18T13:59:49.098876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=256, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the text and get only the assistant's response\nresponse = text.split(\"assistant\")[-1].strip()\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T13:59:49.101075Z","iopub.execute_input":"2024-10-18T13:59:49.101392Z","iopub.status.idle":"2024-10-18T14:00:24.567849Z","shell.execute_reply.started":"2024-10-18T13:59:49.101358Z","shell.execute_reply":"2024-10-18T14:00:24.566869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:00:24.569187Z","iopub.execute_input":"2024-10-18T14:00:24.569597Z","iopub.status.idle":"2024-10-18T14:01:56.498476Z","shell.execute_reply.started":"2024-10-18T14:00:24.569561Z","shell.execute_reply":"2024-10-18T14:01:56.497561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1, temperature=0.8)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the text and get only the assistant's response\nresponse = text.split(\"assistant\")[-1].strip()\n\n\ndisplay(\n    Markdown(\n        response\n            )\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:11:33.853956Z","iopub.execute_input":"2024-10-18T14:11:33.854272Z","iopub.status.idle":"2024-10-18T14:12:44.412519Z","shell.execute_reply.started":"2024-10-18T14:11:33.854237Z","shell.execute_reply":"2024-10-18T14:12:44.411596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I don't feel well these days, I feel I'm so stressed and have a lot of to do when compare myself to my peers, I'm thinkning of ending my life\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1, temperature=0.8)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the text and get only the assistant's response\nresponse = text.split(\"assistant\")[-1].strip()\n\n\ndisplay(\n    Markdown(\n        response\n            )\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:30:46.113632Z","iopub.execute_input":"2024-10-18T14:30:46.114669Z","iopub.status.idle":"2024-10-18T14:31:57.052197Z","shell.execute_reply.started":"2024-10-18T14:30:46.114624Z","shell.execute_reply":"2024-10-18T14:31:57.051272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=256, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the text and get only the assistant's response\nresponse = text.split(\"assistant\")[-1].strip()\n\n\ndisplay(\n    Markdown(\n        response\n            )\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:33:38.356661Z","iopub.execute_input":"2024-10-18T14:33:38.357515Z","iopub.status.idle":"2024-10-18T14:34:13.698040Z","shell.execute_reply.started":"2024-10-18T14:33:38.357469Z","shell.execute_reply":"2024-10-18T14:34:13.697050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I don't feel well these days, I feel I'm so stressed and have a lot of to do when compare myself to my peers, I'm thinkning of ending my life\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=512, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Split the text and get only the assistant's response\nresponse = text.split(\"assistant\")[-1].strip()\n\n\ndisplay(\n    Markdown(\n        response\n            )\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:37:53.101256Z","iopub.execute_input":"2024-10-18T14:37:53.101666Z","iopub.status.idle":"2024-10-18T14:39:03.348638Z","shell.execute_reply.started":"2024-10-18T14:37:53.101625Z","shell.execute_reply":"2024-10-18T14:39:03.347709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T14:39:38.592790Z","iopub.execute_input":"2024-10-18T14:39:38.593464Z","iopub.status.idle":"2024-10-18T14:39:38.599222Z","shell.execute_reply.started":"2024-10-18T14:39:38.593421Z","shell.execute_reply":"2024-10-18T14:39:38.598340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}